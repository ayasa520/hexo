---
title:  数据科学基础(九) 回归分析和方差分析
category: 'NJU 笔记'
tag: ['概率论','数学']
date: 2020-12-27
---

{% note blue ' fas fa-rocket' modern %}
📚 文档目录
<a href="/2020/12/27/数据科学基础/数据科学基础_01">随机事件及其概率</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_02">随机变量及其分布</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_03">期望和方差</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_04">大数定律与中心极限定理</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_05">数理统计的基本概念</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_06">参数估计</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_07">假设检验</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_08">多维</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_09">回归分析和方差分析</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_10">降维</a>
{% endnote %}


## 9.1 回归分析

### 9.1.1 相关性分析

+ 皮尔逊 (Pearson) 相关系数.
  $$
  r=\frac{1}{n-1}\sum_{i=1}^{n}\frac{(X_i-\bar X)(Y_i-\bar Y)}{s_Xs_Y}
  $$
  $\bar X,\bar Y$ 为样本均值, $s_x,s_y$ 是样本方差.

  + Pearson 相关系数用于度量两个随机变量 $X,Y$ 的**线性关系**. 可近似估计 $\rho$ .
  + 取值范围: $[-1,1]$ , 绝对值越接近 1 , 则线性关系越强.
  + 对称性.
  + 原样本经过线性变换不影响 $r$ 值.
  + 不描述因果关系.

+ 对相关系数 $r$ 进行显著性检验
  $$
  H_0:\rho = 0, H_1:\rho\neq 0
  $$
  构造统计量:

  $$
  \begin{aligned}\\
    t&=\frac{r}{S_r}\sim t(n-2), S_r = \sqrt{\frac {1-r^2}{n-2} }
    \end{aligned}
  $$

  若原假设成立, $t$ 值应小, 所以拒绝域为 $|t| > t_{\frac \alpha 2}(n-2)$.

+ 斯皮尔曼( Spearman ) 相关系数:

  将原始数据根据其在总体数据中的平均降序位置分配一个等级 ( rank ), 这些等级变量之间的 Pearson 相关系数就是 Spearman 相关系数.

  例子:
  
  <div style="text-align:center"><img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image-20210502235007093.webp" alt="image-20210502235007093" style="zoom:67%;" /></div>
  
  根据右边表格, 按照下面公式计算 (皮尔逊相关系数展开就是这个):
  $$
  r_{}=\frac{\sum x_{i} y_{i}-\frac{\left(\sum x_{i}\right)\left(\sum y_{i}\right)}{n}}{\sqrt{\sum x_{i}^{2}-\frac{\left(\sum x_{i}\right)^{2}}{n}} \sqrt{\sum y_{i}^{2}-\frac{\left(\sum y_{i}\right)^{2}}{n}}}
  $$

### 9.1.2 一元线性回归分析

#### 概述

对从总体 $$(x, Y)$$ 中抽取的一个样本 $$\left(x_{1}, Y_{1}\right),\left(x_{2}, Y_{2}\right), \ldots,\left(x_{n}, Y_{n}\right)$$
一元线性回归模型：

<div style="text-align:center"><img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image-20210502235759134.webp" alt="image-20210502235759134" style="zoom:67%;" /></div>

根据样本估计 $\beta_0,\beta_1$, 记作 $\hat \beta_0,\hat\beta_1$, 称为 $y$ 关于 $x$ 的一元线性回归
$$
\hat y = \hat \beta_0+ \hat \beta_1 x
$$
一元线性回归要解决的问题

+ 参数估计
  + $$\beta_{0}, \beta_{1}$$ 的估计
  + $\sigma^{2}$ 的估计
+ 参数检验及模型应用
  + 线性假设的显著性检验
  + 回归系数 $$\beta_1$$ 的置信区间
  + Y 的点估计

#### 参数估计

+ $\beta_0,\beta_1$ 的估计 (采用最小二乘法)

  求 $$\hat \beta_0,\hat \beta_1$$ 使  $$\displaystyle Q\left(\hat{\beta}_{0}, \hat{\beta}_{1}\right)=\min _{\alpha,\space \beta} Q\left(\beta_{0}, \beta_{1}\right)$$.

  其中 $$Q(\beta_{0},\beta_1)$$ 是偏差平方和 $$\displaystyle \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)^{2}$$.

  求导令导数为零:
  $$
  \begin{aligned}
  \frac{\partial Q}{\partial \beta_{0}} &=-2 \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)=0 \\
  \frac{\partial Q}{\partial \beta_{1}} &=-2 \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right) x_{i}=0
  \end{aligned}
  $$
  整理一下, 得到正规方程系数行列式:
  $$
  \begin{aligned}
  n \beta_{0}&+\left(\sum_{i=1}^{n} x_{i}\right) \beta_{1}=\sum_{i=1}^{n} y_{i} \\
  \left(\sum_{i=1}^{n} x_{i}\right) \beta_{0}&+\left(\sum_{i=1}^{n} x_{i}^{2}\right) \beta_{1}=\sum_{i=1}^{n} x_{i} y_{i}
  \end{aligned}
  $$
  记:
  $$
  \begin{aligned}
  &\bar{y}=\frac{1}{n} \sum_{i} y_{i}, \bar{x}=\frac{1}{n} \sum_{i} x_{i},& s_{x x}=\sum_{i}\left(x_{i}-\bar{x}\right)^{2} \\
  &s_{x y}=\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right), &s_{yy                                                                                                                                                                                                                                                                                            }=\sum_{i}\left(y_{i}-\bar{y}\right)^{2}
  \end{aligned}
  $$
  可以由正规方程系数行列式得到等式:
  
  $$
  \begin{array}{l}
  \hat{\beta}_{0}+\bar{x} \hat{\beta}_{1}=\bar{y} \\
  s_{x x} \hat{\beta}_{1}=s_{x y}
  \end{array}
  $$

  则 $\beta_0,\beta_1 $ 的最小二乘估计为

  $$
  \begin{aligned}
  \hat{\beta}_{0}=\bar{y}-\bar{x} \hat{\beta}_{1}\\
  \hat{\beta}_{1}=s_{x y} / s_{x x}
  \end{aligned}
  $$

+ 误差 $\sigma^2$ 的估计

  **残差**: $e_i = y_i-\hat y_i$, 残差 $e_i$ 是 $\varepsilon_i$ 的估计.

  由于 $D(\varepsilon_i) = E(\varepsilon_i^2) = \sigma^2$

  想到用**残差平方和**估计随机误差项的方差, 经计算, $\sigma^2$ 的无偏估计为:
  $$
  s^2 = \frac 1 {n-2} \sum_{i=1}^{n}(y_i-\hat y_i)^2
  $$

## 9.2 方差分析

### 9.2.1 单因素方差分析

#### 1.  检验假设

用于推断两个或两个以上总体均值是否有差异的显著性检验.

+ 在方差分析中, 把所考察的试验结果称为**试验指标**.
+ 对试验指标产生影响的原因称为**因素**.
+ 因素的各个不同状态称为**水平**.

对于样本:

<div style="text-align:center"><img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image-20210503000648946.webp" alt="image-20210503000648946" style="zoom:67%;" /></div>

各个样本间是独立的, 则

<div style="text-align:center"><img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image-20210503000737757.webp" alt="image-20210503000737757" style="zoom:67%;" /></div>

记
$$
\sum_{i=1}^{r} n_{i}=n, \bar{X}_{i \bullet}=\frac{1}{n_{i}} \sum_{j=1}^{n_{i}} X_{i j}, \bar{X}=\frac{1}{n} \sum_{i=1}^{r} \sum_{j=1}^{n_{i}} X_{i j}
$$

检验假设:
$$
\begin{aligned}\\
&H_o: \mu_1=\mu_2=...=\mu_r\\
&H_1: \mu_1,\mu_2...\mu_r \,\text{imperfect}\, \text{equality}
\end{aligned}
$$
假设检验采用的方法: 平方和分解:

+ **总偏差平方和 $S_T$:** $$\displaystyle S_T =\sum_{i=1}^{r}\sum_{j=1}^{n_i}(X_{ij}-\bar{X})^2$$
+ **效应平方和:$S_A$:** $$\displaystyle S_A=\sum_{i=1}^{r}n_i(\bar X_{i\bullet}-\bar{X})^2$$
+ **误差平方和$S_E$:** $$\displaystyle S_E = \sum_{i=1}^{r}\sum_{j=1}^{n_i}(X_{ij}-\bar X_{i\bullet})^2$$

定理:

1. $S_T = S_A+S_E$

2. $\frac{S_{E}}{\sigma^{2}} \sim \chi^{2}(n-r)$

   证明:
   $$
   \begin{aligned}\\
   \frac{(n_i-1)\cdot\frac{\sum_{j=1}^{n_i}(X_{ij}-\bar X_{i\bullet})^2}{n_i-1}}{\sigma^2}\sim \chi^2(n_i-1)
   \end{aligned}
   $$

   卡方分布可以叠加

3. $$\begin{array}{l}S_{A} \text { 与 } S_{E} \text { 相互独立, 当 } H_{0} \text { 成立时, } \frac{S_{A}}{\sigma^{2}} \sim \chi^{2}(r-1) \text { , 此时, }\\F=\frac{S_{A} /(r-1)}{S_{E} /(n-r)} \sim F(r-1, n-r), \text { 因为当拒绝原假设时 }\\S_{A} \text { 会偏大, 所以当 } F \geq F_{\alpha}(r-1, n-r)\end{array}
$$


单因素试验方差分析表:
<img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image.webp" style="zoom:100%;" alt="单因素试验方差分析表"/>

例: 保险公司为了解某一险种在四个不同地区索赔额情况是否存在差异。搜集了这四个不同地区一年的索赔额情况记录如表所示. 试判断在四个不同地区索赔额有无显著的差异?

<img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image (1).webp" style="zoom:100%;" alt=""/>

+ 索赔额差异来源于两个方面:

  + 地区之间的差异
  + 同一地区内的随机因素

+ 因素: 地区

+ 水平: 四个不同的地区

+ 最终的方差分析表:

  <img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image (2).webp" style="zoom:100%;" />

#### 2.  未知参数的估计

+ $$\sigma^2的无偏估计为\displaystyle \hat \sigma=\frac {S_E} {n-r}$$.  
+ $$\mu_i的无偏估计为\displaystyle \hat{\mu}_i={\bar {X}_{i\bullet}},i=1,2,\cdots,n_i$$.  

#### 3. 比较 在**部分相等**的情况, 比较的方法有两个

+ 作 $\mu_i - \mu_j(i \ne j)$ 的区间估计

  <div style="text-align:center"><img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image-20210503001132577.webp" alt="image-20210503001132577" style="zoom:50%;" /></div>
  
  求得置信区间, 若置信区间包含零, 则认为没有显著差异.
  
+ 做 $$H_0: \mu_i = \mu_j, H_1:\mu_i\ne \mu_j$$ 的假设检验

  构造检验统计量 $$\displaystyle t_{i j}=\frac{\bar{X}_{i \bullet}-\bar{X}_{j \bullet}}{\sqrt{M S_{E}\left(1 / n_{i}+1 / n_{j}\right)}} ,$$

  原假设成立时, $$\displaystyle t_{i j} \sim t(n-r),$$

  拒绝域 $|t_{ij}|\geq t_{\alpha/2}(n-r)$

### 9.2.2 双因素试验的方差分析

略

### 9.2.3 双因素试验的方差分析

略

## 9.3 正交试验设计

### 极差分析

仅有一个例子

<div style="text-align:center"><img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image-20210503001402429.webp" alt="image-20210503001402429" style="zoom:67%;" /></div>

$A,B,C$ 下每个单元格内容为该因素的水平.

<div style="text-align:center"><img src="https://cdn.jsdelivr.net/gh/ayasa520/ayasa520.github.io/image/数据科学基础_9.assets/image-20210503001443516.webp" alt="image-20210503001443516" style="zoom:50%;" /></div>

找到所有该因素对应水平下的试验指标, 求和填入. 可知 $A$ 因素影响最为显著, $C$ 最不显著. 最佳组合为 $A:3, B:1, C:1$
