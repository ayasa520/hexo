---
title:  数据科学基础(十) 降维
category: 'NJU 笔记'
tag: ['概率论','数学']
date: 2020-12-27
---

{% note blue ' fas fa-rocket' modern %}
📚 文档目录
<a href="/2020/12/27/数据科学基础/数据科学基础_01">随机事件及其概率</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_02">随机变量及其分布</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_03">期望和方差</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_04">大数定律与中心极限定理</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_05">数理统计的基本概念</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_06">参数估计</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_07">假设检验</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_08">多维</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_09">回归分析和方差分析</a>
<a href="/2020/12/27/数据科学基础/数据科学基础_10">降维</a>
{% endnote %}



## 10.1 主成分分析（PCA)

不懂线性代数, 下面这些参考了一些 PCA 的说明, 但我总觉得某些解释的不是很严谨.

#### 目标

PCA 常用于高维数据的降维,可用于提取数据的主要特征分量.

对于原始数据矩阵 
$$
A=\begin{bmatrix}
 x_{11} &x_{12}  & \cdots & x_{1n}\\
 x_{21} &x_{22}  & \cdots & x_{2n}\\
\vdots & \vdots  & \cdots &\vdots \\
 x_{r1} &x_{r2}  & \cdots & x_{rn}\\
\end{bmatrix}
$$
其中, 列向量$$(x_{1i},x_{2i},\cdots,x_{ri})^T$$ 为 $n$ 个样本中的一个. $r$ 行表示 $r$ 个维度.

对该矩阵进行中心化,得到中心化矩阵 $X$

<div style="text-align:center"><img src="https://unpkg.zhimg.com/rikka-os@1.0.3/img/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%9F%BA%E7%A1%80_10.assets/image-20210503001820524.webp alt="image-20210503001820524" style="zoom:67%;" /></div>

 X 中心化后, 样本点的中心点即原点, 寻找点分散程度最大的方向, 即让这些点投影后的分散程度最大.

<div style="text-align:center"><img src="https://unpkg.zhimg.com/rikka-os@1.0.3/img/数据科学基础_10.assets/8fc21d1f6638e0ef2f83a4d16b6f30b6c04e756e.gif" title="如图所示" style="zoom:100%;" /></div>

#### 向量内积

若 $\alpha = (a_1, a_2,\cdots,a_n)^T,\beta = (b_1,b_2,\cdots,b_n)^T$ , 则内积可表示为:

$$
\alpha \cdot \beta= \alpha^T \beta =a_1b_1+a_2b_2+ \cdots +a_nb_n
$$

内积的几何意义:
$$
A\cdot B = |A|\cdot|B|\cos\theta
$$
当 $B$ 为单位向量$(\sqrt{b_1^2+b_2^2+\cdots+b_n^2}=1)$时, 两个向量的内积就是 $A$ 在这个单位向量方向投影的长度.



#### 散度

分散程度可以用方差或者协方差衡量,  回顾一下方差和协方差:
$$
\begin{aligned}\\
&&s^{2}(X)&=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}{n-1}\\
&&{Cov}(X, Y)&=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{n-1}\\
\end{aligned}
$$
构建协方差矩阵 $C$:
$$
C=\begin{bmatrix}
cov(X_1, X_1) & cov(X_1, X_2) &\cdots &cov(X_1, X_n) \\
cov(X_2, X_1) &cov(X_2, X_2) & \cdots&cov(X_2, X_n) \\
\vdots & \vdots  & \cdots &\vdots \\
cov(X_n, X_1) & cov(X_n, X_2) &\cdots &cov(X_n, X_n)
\end{bmatrix}
$$
由上述公式可知协方差矩阵 $C$ 的每一项为:
$$
C_{ij} =cov(X_i,X_j) = \frac{\sum_{k=1}^{n}\left(X_{ik}-\bar{X_i}\right)\left(X_{jk}-\bar{X_j}\right)}{n-1}= \frac {X_{i1}X_{j1}+X_{i2}X_{j2}+\cdots+X_{in}X_{jn}}{n-1}
$$
刚好是 $Z$ 中的第 $i$ 行与第 $j$ 行做内积再除以 $n-1$ 的结果.

则协方差矩阵与中心化后的原始数据矩阵存在以下关联:
$$
C=\frac 1 {n-1} XX^T
$$
设要投影的单位向量为 $V$ , 则得到的投影后的值为$V\cdot Z=V^TZ$, 投影后的方差为:
$$
s^2 = \frac 1 {n-1} \sum_{i=1}^n (V^T \alpha_i- \frac 1 n\sum _{i=1} ^n(V^T\alpha_i))^2=\frac 1 {n-1} \sum_{i=1}^n V^T \alpha_i \alpha_i^TV = \frac{1}{n-1} V^TZZ^TV=V^TCV
$$
其中 $\alpha_i$ 为 $Z$ 中的第 $i$ 列.

#### 拉格朗日乘数法

求 $S^2$ 最大值,限制条件: $||V||=1$

构建方程:

$$
F(V) = V^TCV-\lambda(V^TV-1)①
$$

对 $V$ 求导数得:

$$
\frac {\partial F}{\partial V} = 2CV-2\lambda V
$$

令导数为零得:

$$
CV=\lambda V ②
$$

这个形式是特征值和特征向量的定义式, $C$ 是 $n$ 阶方阵, $V$ 是特征向量, $\lambda$ 是特征值. 求特征值和特征向量需要进行特征值分解 (EVD) , 这是线性代数的内容.



#### 降维

将 ② 代入 ① 得 $ s^2 = F(V) = \lambda$, 特征值 $λ$  越大, 则散度越大.

将所有的特征值降序排列, 根据最终需要的维度 $d$ 来选择前 $d$ 大的特征值对应的特征向量, 并将特征向量单位化后组成矩阵 $W = (w_1,w_2,\cdots,w_d)$, 由于每个点都可以视为在各个特征向量方向上的投影组成, 则最终降维后:
$$
X_{d\times n} = W^TX = 
\begin{pmatrix}
w1^T\\
w2^T\\
\vdots\\
w_d^T\\
\end{pmatrix}
X
$$
矩阵 $X_{d\times n}$ 的第一行称为第一主成分, 以此类推.

**关于 d 的选择:**

按 czy 课件来的话, 要求 $$\displaystyle \frac{ \sum _{i=1}^d \lambda_i}{\sum_{i=1}^p \lambda_i}\geq 0.85$$ , p 为得到的特征值的数量.


